{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec7dad4",
   "metadata": {},
   "source": [
    "$Loan$ $Eligibility$ $Prediction$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafdc8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c5f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(name: str, log_filename: str | Path, level = logging.INFO) -> logging.Logger:\n",
    "    ''' Setup a dedicated timedrotatingfilehandler logging system that logs information to both file and console\n",
    "\n",
    "    Args: \n",
    "        name : logger name (e.g. EDA, preprocessing, feature_engineering)\n",
    "        log_filename: Log output file\n",
    "        level: Logging level (e.g. INFO, WARNING, ERROR, DEBUG)\n",
    "\n",
    "    Examples:\n",
    "        log = setup_logger(name=\"EDA\",log_filename=\"logs/EDA_pipeline.log\", level=logging.INFO)\n",
    "        log.info(\"Dedicated logging system setup successful\")\n",
    "    '''\n",
    "    log = logging.getLogger(name)\n",
    "    # prevent adding handlers multiple times if handlers already exist\n",
    "    if log.handlers:\n",
    "        return log\n",
    "    \n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(levelname)s : %(message)s\",\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "    # Time rotating file handler\n",
    "    file_handler = TimedRotatingFileHandler(\n",
    "        filename=log_filename,\n",
    "        when='midnight',\n",
    "        interval=1,\n",
    "        backupCount=7\n",
    "    )\n",
    "    file_handler.suffix = \"_%Y%m%d\"\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    log.propagate = False # don't propagate to root logger\n",
    "    log.setLevel(level)\n",
    "\n",
    "    log.addHandler(file_handler)\n",
    "    log.addHandler(console_handler)\n",
    "    \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8907abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = setup_logger(name='notebook_eda', log_filename='../logs/notebook_eda.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71919b9",
   "metadata": {},
   "source": [
    "`Basic Descriptive Summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782cec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('../data/raw/LEP.csv')\n",
    "except FileNotFoundError:\n",
    "    log.error('File not found! Check filepath and try again')\n",
    "    raise\n",
    "except Exception as e:\n",
    "    log.error(f'Error parsing CSV file: {e}',exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22de368",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777b8619",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "summary = df[numeric_cols[1:]].describe().T\n",
    "summary['range'] = summary['max'] - summary['min']\n",
    "summary['cv'] = round(summary['mean'] / summary['std'],4)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36077f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Education'].value_counts().head(3).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9109ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "summary_data = []\n",
    "for col in categorical_cols:\n",
    "    summary_data.append({'column' : col,\n",
    "    'unique' : df[col].nunique(),\n",
    "    'most_frequent' : df[col].mode()[0] if len(df[col].mode()) > 0 else None,\n",
    "    'most_frequent_count' : df[col].value_counts().iloc[0] if len(df[col]) > 0 else 0,\n",
    "    'top_5_values' : df[col].value_counts().head().to_dict()\n",
    "    })\n",
    "summary_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e6e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2781ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(f'Number of observations: {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4be2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(f'Number of features: {df.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(exclude='object').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38776dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(exclude=np.number).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528e8819",
   "metadata": {},
   "source": [
    "`Numerical Columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec048b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "log.info(f'NUMERICAL COLUMNS')\n",
    "log.info('='*30)\n",
    "log.info(numeric_columns)\n",
    "log.info('='*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a439e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('='*50)\n",
    "for i, col in enumerate(numeric_columns,1):\n",
    "    log.info(f'{i}. {col:<20} | Min: {df[col].min():<7} | Max: {df[col].max()}')\n",
    "log.info('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5daeeb",
   "metadata": {},
   "source": [
    "`Categorical Columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d392b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('CATEGORICAL COLUMNS')\n",
    "categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "log.info('='*30)\n",
    "log.info(categorical_cols)\n",
    "log.info('='*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('='*30)\n",
    "for i, col in enumerate(categorical_cols,1):\n",
    "    uniques = df[col].unique()\n",
    "    log.info(f'{i}. {col:<15} | Unique : {df[col].nunique()} | Examples: {uniques[:6]}')\n",
    "log.info('='*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1aafb9",
   "metadata": {},
   "source": [
    "`Missing Values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a77058",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'missing_values' : missing,\n",
    "    'missing_pct' : missing_pct.round(2)\n",
    "})\n",
    "\n",
    "log.info('MISSING VALUES')\n",
    "log.info('='*30)\n",
    "log.info(missing_df)\n",
    "log.info('='*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca443d2",
   "metadata": {},
   "source": [
    "`Duplicate Values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8305333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated()]\n",
    "log.info(f'DUPLICATED VALUES')\n",
    "log.info('='*30)\n",
    "log.info(f'Number of duplicated values : {len(duplicates)}')\n",
    "log.info('='*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e9a15e",
   "metadata": {},
   "source": [
    "`Handling Outlier Values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('OUTLIER SUMMARY')\n",
    "log.info('='*35)\n",
    "for i,col in enumerate(numeric_columns[1:],1): # ignore customerID column - just an identifier\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    \n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    log.info(f'{i}. {col:<20} | Number of outliers : {len(outliers):<2} | Range ({lower_bound} - {upper_bound})')\n",
    "log.info('='*35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3433b34",
   "metadata": {},
   "source": [
    "$Visualizations$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7910b19e",
   "metadata": {},
   "source": [
    "`Univariate Analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046abf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.use('agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(style=\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_context(context='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = len(numeric_cols[1:])\n",
    "n_rows = (n_cols + 2) // 3\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, 3, figsize=(15,10))\n",
    "axes = axes.flatten() if len(n_cols) > 1 else [axes]\n",
    "\n",
    "for idx, col in enumerate(numeric_cols[1:]):\n",
    "    try:\n",
    "        sns.histplot(data=df, x=col, kde=True, ax=axes['idx'],color='purple', alpha=0.7)\n",
    "        axes[idx].set_title(f'Distribution of {col}')\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    except Exception as e:\n",
    "        log.exception(f'Error plotting distribution: {e}')\n",
    "\n",
    "for idx in range(n_cols, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68789d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('='*50)\n",
    "log.info('NUMERIC COLUMNS - VISUALIZATIONS')\n",
    "log.info('='*50)\n",
    "plt.figure(figsize=(18,10))\n",
    "for i,col in enumerate(numeric_columns[1:],1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.histplot(data=df, x=col, color='purple', alpha=0.7,kde=True)\n",
    "    plt.title(f'Distribution of {col}', fontsize=13, fontweight='bold')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "log.info(f'Distribution of numeric columns plotted!')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73314ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('='*50)\n",
    "log.info('BOXPLOTS - OUTLIER DETECTION')\n",
    "log.info('='*50)\n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "for i,col in enumerate(numeric_columns[1:],1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.boxplot(data=df, y=col, color='gold', linecolor='black')\n",
    "    plt.title(f'Boxplot - {col}', fontsize=13, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "log.info(f'Boxplots of numeric columns plotted!')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc587bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('='*50)\n",
    "log.info('CATEGORICAL COLUMNS')\n",
    "log.info('='*50)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "for i, col in enumerate(categorical_cols,1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    ax = sns.countplot(data=df, x=col, gap=0.4, width=0.5, saturation=0.8, color='green')\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, label_type='edge')\n",
    "    ax.set_title(f'{col}')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "log.info(f'Categorical columns plotting successful')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f9ce7",
   "metadata": {},
   "source": [
    "`Multivariate Analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('='*50)\n",
    "log.info('CORRELATION HEATMAP')\n",
    "log.info('='*50)\n",
    "\n",
    "corr = df.corr(numeric_only=True, method='spearman')\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(data=corr,annot=True,fmt='.2f',cmap='Blues')\n",
    "plt.title(f'Correlation heatmap', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "log.info(f'Heatmap successfully plotted')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6836d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('='*30)\n",
    "log.info('TARGET VARIABLE')\n",
    "log.info('='*30)\n",
    "\n",
    "group_values = df['Loan_Status'].value_counts().to_dict()\n",
    "for group, value in group_values.items():\n",
    "    log.info(f'{group} : {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49ae43c",
   "metadata": {},
   "source": [
    "`Confidence Interval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def calculate_ci(data, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate confidence interval for a numerical feature\n",
    "    \n",
    "    Args:\n",
    "        data: array-like numerical data\n",
    "        confidence: confidence level (default 0.95 for 95% CI)\n",
    "    \n",
    "    Returns:\n",
    "        dict with mean, lower_bound, upper_bound, margin_of_error\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    se = stats.sem(data)  # standard error\n",
    "    margin = se * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'lower_bound': mean - margin,\n",
    "        'upper_bound': mean + margin,\n",
    "        'margin_of_error': margin,\n",
    "        'sample_size': n\n",
    "    }\n",
    "\n",
    "log.info('='*60)\n",
    "log.info('CONFIDENCE INTERVALS FOR NUMERICAL FEATURES')\n",
    "log.info('='*60)\n",
    "\n",
    "# Example: Calculate 95% CI for all numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col != 'Customer_ID']  # exclude ID\n",
    "\n",
    "for col in numeric_cols:\n",
    "    ci = calculate_ci(df[col].dropna())\n",
    "    log.info(f\"\\n{col}:\")\n",
    "    log.info(f\"  Mean: {ci['mean']:.2f}\")\n",
    "    log.info(f\"  95% CI: [{ci['lower_bound']:.2f}, {ci['upper_bound']:.2f}]\")\n",
    "    log.info(f\"  Margin of Error: Â±{ci['margin_of_error']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu, ttest_ind\n",
    "def compare_groups_ttest(df, numeric_col, grouping_col, group1_val, group2_val):\n",
    "    ''' \n",
    "    Perform independent t-test between two groups\n",
    "\n",
    "    H0 : Average of group1 = Average of group2\n",
    "    H1 : Average of group1 != Average of group2\n",
    "    '''\n",
    "\n",
    "    group1 = df[df[grouping_col] == group1_val][numeric_col].dropna()\n",
    "    group2 = df[df[grouping_col] == group2_val][numeric_col].dropna()\n",
    "\n",
    "    # check for normality (shapiro-wilk test)\n",
    "    _, p_value1 = stats.shapiro(group1.sample(min(5000, len(group1))))\n",
    "    _, p_value2 = stats.shapiro(group2.sample(min(5000, len(group2))))\n",
    "\n",
    "    if p_value1 < 0.05 or p_value2 < 0.05:\n",
    "        statistic, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "        test_type = 'mannwhitneyu (non-parametric)'\n",
    "\n",
    "    else:\n",
    "        statistic, p_value = ttest_ind(group1, group2, equal_var=False)\n",
    "        test_type = \"Welsh's t-test\"\n",
    "\n",
    "    return {\n",
    "        'test_type' : test_type,\n",
    "        'group1_mean' : group1.mean(),\n",
    "        'group2_mean' : group2.mean(),\n",
    "        'group1_std' : group1.std(),\n",
    "        'group2_std' : group2.std(),\n",
    "        'statistic' : statistic,\n",
    "        'p_value' : p_value,\n",
    "        'significant' : p_value < 0.05,\n",
    "        'effect_size' : abs(group1.mean() - group2.mean()) / np.sqrt((group1.std()**2 + group2.std()**2) / 2)\n",
    "    }\n",
    "\n",
    "target_value = 'Loan_Status'\n",
    "for col in numeric_cols[1:]:\n",
    "    result = compare_groups_ttest(df, col, target_value, 'Y', 'N')\n",
    "    # Y = loan approved\n",
    "    # N = loan rejected\n",
    "    log.info(f'\\n{col} - {result['test_type']}')\n",
    "    log.info(f'Approved mean: {result['group1_mean']:.2f} +/- ({result['group1_std']:.2f})')\n",
    "    log.info(f'Rejected mean: {result['group2_mean']:.2f} +/- ({result['group2_std']:.2f})')\n",
    "    log.info(f'Test statistic: {result['statistic']:.2f}')\n",
    "    log.info(f'P-Value : {result['p_value']:.2f}')\n",
    "    log.info(f\"Effect size (Cohen's d) : {result['effect_size']:.3f}\")\n",
    "\n",
    "    if result['significant']:\n",
    "        log.info(f'SIGNIFICANT difference: p_value = {result['p_value']:.2f}  < 0.05')\n",
    "    else:\n",
    "        log.info(f'NO significant difference: p_value = {result['p_value']:.2f} > 0.05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d19de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "def chi_square_test(df, cat_cols, target_col):\n",
    "    '''Perform chi-square of independence\n",
    "    \n",
    "    H0 : Categorical feature is independent of target\n",
    "    H1 : Categorical feature is associated of target\n",
    "    '''\n",
    "\n",
    "    contigency_table = pd.crosstab(df[cat_cols], df[target_col])\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contigency_table)\n",
    "\n",
    "    # Craimer's V for effect size\n",
    "    n = contigency_table.sum().sum()\n",
    "    min_dim = min(contigency_table.shape) - 1\n",
    "    cramers_v = np.sqrt(chi2 / (n * min_dim))\n",
    "\n",
    "    return {\n",
    "        'chi2' : chi2,\n",
    "        'p_value' : p_value,\n",
    "        'dof' : dof,\n",
    "        'cramers_v' : cramers_v,\n",
    "        'significant' : p_value < 0.05,\n",
    "        'contigency_table' : contigency_table\n",
    "    }\n",
    "\n",
    "log.info('='*60)\n",
    "log.info('CHI-SQUARE TESTS: CATEGORICAL FEATURES vs TARGET')\n",
    "log.info('='*60)\n",
    "\n",
    "\n",
    "cat_cols = [col for col in categorical_cols if col != target_value]\n",
    "\n",
    "for col in cat_cols:\n",
    "    result = chi_square_test(df, col, target_value)\n",
    "\n",
    "    log.info(f'\\n{col}')\n",
    "    log.info(f'Chi square : {result['chi2']:.4f}')\n",
    "    log.info(f'p_value : {result['p_value']:.4f}')\n",
    "    log.info(f\"Cramer's V : {result['cramers_v']:.4f}\")\n",
    "    \n",
    "    if result['significant']:\n",
    "        log.info(f'SIGNIFICANT difference: p_value = {result['p_value']:.2f}  < 0.05')\n",
    "    else:\n",
    "        log.info(f'NO significant difference: p_value = {result['p_value']:.2f} > 0.05')\n",
    "        \n",
    "    print(f'\\nContigency Table')\n",
    "    print(f'{result['contigency_table']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af25cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "def multi_group_comparison(df, grouping_col, numeric_col):\n",
    "    ''' \n",
    "    Compare numerical feature across multiple categorical groups\n",
    "\n",
    "    H0 : All group means are equal\n",
    "    H1 : At least one group mean is different\n",
    "    '''\n",
    "    groups = [group[numeric_col].dropna() for name, group in df.groupby(grouping_col)]\n",
    "\n",
    "    # check for normality\n",
    "    normality_p_values = [stats.shapiro(g.sample(min(5000, len(g))))[1] for g in groups]\n",
    "\n",
    "    if all(p >= 0.05 for p in normality_p_values):\n",
    "        statistic, p_value = stats.f_oneway(*groups)\n",
    "        test_type = 'one-way ANOVA test'\n",
    "    \n",
    "    else:\n",
    "        statistic, p_value = kruskal(*groups)\n",
    "        test_type = 'Kruskal-Wallis H-test'\n",
    "\n",
    "    return {\n",
    "        'test_type' : test_type,\n",
    "        'statistic' : statistic,\n",
    "        'p_value' : p_value,\n",
    "        'significant' : p_value < 0.05,\n",
    "        'groupby_means' : df.groupby(grouping_col)[numeric_col].mean().to_dict()\n",
    "    }\n",
    "\n",
    "log.info('='*60)\n",
    "log.info('MULTI-GROUP COMPARISONS (e.g., Income across Education Levels)')\n",
    "log.info('='*60)\n",
    "\n",
    "if 'Education' and 'Applicant_Income' in numeric_columns:\n",
    "    result = multi_group_comparison(df, 'Education', 'Applicant_Income')\n",
    "\n",
    "    log.info(f'\\nIncome Across Education levels : {result['test_type']}')\n",
    "    log.info(f'Test statistics : {result['statistic']:.2f}')\n",
    "    log.info(f'p_value : {result['p_value']:.2f}')\n",
    "    \n",
    "\n",
    "    log.info(f'\\nGroup Means')\n",
    "    for group, mean in result['groupby_means'].items():\n",
    "        log.info(f'{group} : {mean:.2f}')\n",
    "\n",
    "    if result['significant']:\n",
    "        log.info(f'SIGNIFICANT difference: p_value = {result['p_value']:.2f}  < 0.05')\n",
    "    else:\n",
    "        log.info(f'NO significant difference: p_value = {result['p_value']:.2f} > 0.05')       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
